{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "601fb41e",
   "metadata": {},
   "source": [
    "# MACHINE LEARNING — ASSIGNMENT 2 (Template)\n",
    "\n",
    "**Topic:** Multi-model classification + Streamlit deployment  \n",
    "**Planned Dataset:** Credit Card Default (UCI/Kaggle)\n",
    "\n",
    "> Use this notebook as your *single source of truth* while developing.  \n",
    "> Later, you can copy core code into `model/` scripts and your `app.py`.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92dd1fd4",
   "metadata": {},
   "source": [
    "## STUDENT INFORMATION (REQUIRED — DO NOT DELETE)\n",
    "\n",
    "Fill these before submission.\n",
    "\n",
    "- **Name:** HARISH BHATT  \n",
    "- **Program:** M.Tech (AIML/DSE)  \n",
    "- **Course:** Machine Learning  \n",
    "- **Assignment:** 2  \n",
    "- **Submission Date:** 15-Feb-2026  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5508bd91",
   "metadata": {},
   "source": [
    "## SUBMISSION REQUIREMENTS CHECKLIST (Keep visible while working)\n",
    "\n",
    "You must eventually submit a **single PDF** containing, in order:\n",
    "1. GitHub repo link (code + `requirements.txt` + `README.md`)\n",
    "2. Live Streamlit app link\n",
    "3. Screenshot proof from BITS Virtual Lab\n",
    "4. Paste the full README content into the PDF\n",
    "\n",
    "This notebook helps you generate the code + results needed for README/Streamlit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ec645b",
   "metadata": {},
   "source": [
    "## 1) Dataset Selection, Loading & Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124f647e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REQUIRED: Fill these metadata fields (used later in README)\n",
    "DATASET_NAME = \"Default of Credit Card Clients\"\n",
    "DATASET_SOURCE = \"UCI / Kaggle\"   # paste the exact URL in README\n",
    "TARGET_COL = \"default payment next month\"  # confirm exact column name in your file\n",
    "\n",
    "# Path to dataset file\n",
    "# - Kaggle: usually CSV\n",
    "# - UCI: XLS (requires read_excel(header=1))\n",
    "DATA_PATH = \"data/credit_card_default.csv\"  # <-- update\n",
    "\n",
    "print(\"DATASET METADATA\")\n",
    "print(\"Name   :\", DATASET_NAME)\n",
    "print(\"Source :\", DATASET_SOURCE)\n",
    "print(\"Target :\", TARGET_COL)\n",
    "print(\"Path   :\", DATA_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b028d643",
   "metadata": {},
   "source": [
    "## 1) Imports & Environment Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f50dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If running in Colab, uncomment and install dependencies as needed\n",
    "# !pip -q install pandas numpy scikit-learn xgboost matplotlib seaborn joblib\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, roc_auc_score,\n",
    "    precision_score, recall_score,\n",
    "    f1_score, matthews_corrcoef,\n",
    "    confusion_matrix, classification_report\n",
    ")\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import joblib\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "print(\"Python:\", sys.version.split()[0])\n",
    "print(\"pandas:\", pd.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8cbb04",
   "metadata": {},
   "source": [
    "## 2) Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ca779f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option A: Kaggle CSV\n",
    "DATA_PATH = \"data/credit_card_default.csv\"  # <-- update this path\n",
    "\n",
    "# Option B: UCI .xls\n",
    "# DATA_PATH = \"data/default of credit card clients.xls\"\n",
    "\n",
    "# If using XLS, use: pd.read_excel(..., header=1) to skip the descriptive row.\n",
    "if DATA_PATH.lower().endswith(\".xls\") or DATA_PATH.lower().endswith(\".xlsx\"):\n",
    "    df = pd.read_excel(DATA_PATH, header=1)\n",
    "else:\n",
    "    df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "print(\"Shape:\", df.shape)\n",
    "display(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0303e9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity checks (REQUIRED)\n",
    "MIN_ROWS = 500\n",
    "MIN_FEATURES = 12\n",
    "\n",
    "print(\"Columns:\", len(df.columns))\n",
    "print(\"Rows   :\", len(df))\n",
    "\n",
    "# Basic shape checks per assignment requirement\n",
    "assert len(df) >= MIN_ROWS, f\"Dataset has {len(df)} rows; need at least {MIN_ROWS}.\"\n",
    "# features check will be done after defining X (excluding target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3385474e",
   "metadata": {},
   "source": [
    "## 3) Quick EDA / Data Checks (Optional but useful)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5aa5054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic info\n",
    "display(df.describe(include='all').T.head(30))\n",
    "print(\"\\nMissing values per column (top 20):\")\n",
    "display(df.isna().sum().sort_values(ascending=False).head(20))\n",
    "\n",
    "# Class balance (update target column name if needed)\n",
    "TARGET_COL = \"default payment next month\"  # <-- confirm exact name\n",
    "if TARGET_COL in df.columns:\n",
    "    display(df[TARGET_COL].value_counts(dropna=False))\n",
    "else:\n",
    "    print(f\"Target column '{TARGET_COL}' not found. Columns are:\", list(df.columns))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83e07ff",
   "metadata": {},
   "source": [
    "## 4) Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f9cb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 Drop ID column if present\n",
    "if \"ID\" in df.columns:\n",
    "    df = df.drop(columns=[\"ID\"])\n",
    "\n",
    "# 4.2 Define X and y\n",
    "TARGET_COL = \"default payment next month\"  # <-- confirm exact name\n",
    "X = df.drop(columns=[TARGET_COL])\n",
    "y = df[TARGET_COL]\n",
    "\n",
    "print(\"X shape:\", X.shape, \"| y shape:\", y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f68499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature/target sanity checks (REQUIRED)\n",
    "\n",
    "assert TARGET_COL in df.columns, f\"Target column not found: {TARGET_COL}\"\n",
    "\n",
    "n_features = X.shape[1]\n",
    "print(\"Number of features (X):\", n_features)\n",
    "\n",
    "assert n_features >= MIN_FEATURES, f\"Dataset has {n_features} features; need at least {MIN_FEATURES}.\"\n",
    "\n",
    "# Target distribution (useful for observations)\n",
    "print(\"\\nTarget distribution:\")\n",
    "display(y.value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83eb58a4",
   "metadata": {},
   "source": [
    "## 5) Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2924b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(\"Train:\", X_train.shape, \"Test:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3faf02",
   "metadata": {},
   "source": [
    "## 6) Scaling (for Logistic Regression + KNN; optional for others)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a008b2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"Scaled shapes:\", X_train_scaled.shape, X_test_scaled.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e80ccc6",
   "metadata": {},
   "source": [
    "## 7) Evaluation Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039c64d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_binary_proba(model, X):\n",
    "    \"\"\"Return P(class=1) for binary classifiers.\"\"\"\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        proba = model.predict_proba(X)[:, 1]\n",
    "    elif hasattr(model, \"decision_function\"):\n",
    "        # Convert decision scores to pseudo-probabilities via min-max scaling as fallback\n",
    "        scores = model.decision_function(X)\n",
    "        proba = (scores - scores.min()) / (scores.max() - scores.min() + 1e-12)\n",
    "    else:\n",
    "        raise ValueError(\"Model has neither predict_proba nor decision_function.\")\n",
    "    return proba\n",
    "\n",
    "\n",
    "def evaluate_model(model, Xtr, Xte, ytr, yte, label_name=None):\n",
    "    \"\"\"Fit, predict, compute required metrics for binary classification.\"\"\"\n",
    "    model.fit(Xtr, ytr)\n",
    "    y_pred = model.predict(Xte)\n",
    "    y_prob = get_binary_proba(model, Xte)\n",
    "\n",
    "    metrics = {\n",
    "        \"Accuracy\": accuracy_score(yte, y_pred),\n",
    "        \"AUC\": roc_auc_score(yte, y_prob),\n",
    "        \"Precision\": precision_score(yte, y_pred, zero_division=0),\n",
    "        \"Recall\": recall_score(yte, y_pred, zero_division=0),\n",
    "        \"F1\": f1_score(yte, y_pred, zero_division=0),\n",
    "        \"MCC\": matthews_corrcoef(yte, y_pred),\n",
    "    }\n",
    "\n",
    "    # Confusion matrix + report\n",
    "    cm = confusion_matrix(yte, y_pred)\n",
    "    report = classification_report(yte, y_pred, digits=4, zero_division=0)\n",
    "\n",
    "    return metrics, cm, report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3117af63",
   "metadata": {},
   "source": [
    "## 8) Train the 6 Required Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1f2946",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "artifacts = {}  # store models + cm/report for later use in Streamlit\n",
    "\n",
    "# 8.1 Logistic Regression (scaled)\n",
    "lr = LogisticRegression(max_iter=2000, random_state=RANDOM_STATE)\n",
    "m, cm, report = evaluate_model(lr, X_train_scaled, X_test_scaled, y_train, y_test)\n",
    "results.append((\"Logistic Regression\", m))\n",
    "artifacts[\"Logistic Regression\"] = {\"model\": lr, \"cm\": cm, \"report\": report}\n",
    "\n",
    "# 8.2 Decision Tree (unscaled)\n",
    "dt = DecisionTreeClassifier(random_state=RANDOM_STATE)\n",
    "m, cm, report = evaluate_model(dt, X_train, X_test, y_train, y_test)\n",
    "results.append((\"Decision Tree\", m))\n",
    "artifacts[\"Decision Tree\"] = {\"model\": dt, \"cm\": cm, \"report\": report}\n",
    "\n",
    "# 8.3 KNN (scaled)\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "m, cm, report = evaluate_model(knn, X_train_scaled, X_test_scaled, y_train, y_test)\n",
    "results.append((\"KNN\", m))\n",
    "artifacts[\"KNN\"] = {\"model\": knn, \"cm\": cm, \"report\": report}\n",
    "\n",
    "# 8.4 Naive Bayes (scaled is fine)\n",
    "nb_model = GaussianNB()\n",
    "m, cm, report = evaluate_model(nb_model, X_train_scaled, X_test_scaled, y_train, y_test)\n",
    "results.append((\"Naive Bayes (Gaussian)\", m))\n",
    "artifacts[\"Naive Bayes (Gaussian)\"] = {\"model\": nb_model, \"cm\": cm, \"report\": report}\n",
    "\n",
    "# 8.5 Random Forest (unscaled)\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=300,\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1\n",
    ")\n",
    "m, cm, report = evaluate_model(rf, X_train, X_test, y_train, y_test)\n",
    "results.append((\"Random Forest\", m))\n",
    "artifacts[\"Random Forest\"] = {\"model\": rf, \"cm\": cm, \"report\": report}\n",
    "\n",
    "# 8.6 XGBoost (unscaled)\n",
    "xgb = XGBClassifier(\n",
    "    n_estimators=400,\n",
    "    max_depth=4,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.9,\n",
    "    colsample_bytree=0.9,\n",
    "    reg_lambda=1.0,\n",
    "    random_state=RANDOM_STATE,\n",
    "    eval_metric=\"logloss\",\n",
    "    n_jobs=-1\n",
    ")\n",
    "m, cm, report = evaluate_model(xgb, X_train, X_test, y_train, y_test)\n",
    "results.append((\"XGBoost\", m))\n",
    "artifacts[\"XGBoost\"] = {\"model\": xgb, \"cm\": cm, \"report\": report}\n",
    "\n",
    "results_df = pd.DataFrame([r[1] for r in results], index=[r[0] for r in results])\n",
    "display(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a92b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results validation (REQUIRED — prevent accidental 0.0 / missing metrics)\n",
    "\n",
    "REQUIRED_MODELS = [\n",
    "    \"Logistic Regression\",\n",
    "    \"Decision Tree\",\n",
    "    \"KNN\",\n",
    "    \"Naive Bayes (Gaussian)\",\n",
    "    \"Random Forest\",\n",
    "    \"XGBoost\"\n",
    "]\n",
    "\n",
    "# Check all required models present\n",
    "missing = [m for m in REQUIRED_MODELS if m not in results_df.index]\n",
    "assert not missing, f\"Missing model results for: {missing}\"\n",
    "\n",
    "# Check for NaNs\n",
    "assert not results_df.isna().any().any(), \"Some metrics are NaN. Check AUC/probabilities.\"\n",
    "\n",
    "# Check metrics not all zeros\n",
    "for col in results_df.columns:\n",
    "    assert (results_df[col] != 0).any(), f\"Column {col} looks all zeros — did metrics compute correctly?\"\n",
    "\n",
    "print(\"✅ Results table contains all required models and valid metric values.\")\n",
    "display(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9d43fc",
   "metadata": {},
   "source": [
    "## 9) Pick a Best Model (Customize your criteria)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16285458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can choose based on AUC, MCC, or F1. Here we pick best by AUC.\n",
    "best_model_name = results_df[\"AUC\"].idxmax()\n",
    "best_model = artifacts[best_model_name][\"model\"]\n",
    "\n",
    "print(\"Best model by AUC:\", best_model_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14786e24",
   "metadata": {},
   "source": [
    "## 10) Visuals: Confusion Matrix & Report (for selected model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5036e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = artifacts[best_model_name][\"cm\"]\n",
    "report = artifacts[best_model_name][\"report\"]\n",
    "\n",
    "print(\"Classification Report:\\n\", report)\n",
    "\n",
    "plt.figure(figsize=(5,4))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cbar=False)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(f\"Confusion Matrix — {best_model_name}\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb52649",
   "metadata": {},
   "source": [
    "## 11) Save Artifacts for Streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35c2d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create folders\n",
    "os.makedirs(\"model\", exist_ok=True)\n",
    "\n",
    "# Save scaler (needed for LR + KNN + NB in this notebook)\n",
    "joblib.dump(scaler, \"model/scaler.pkl\")\n",
    "\n",
    "# Save each model separately\n",
    "for name, obj in artifacts.items():\n",
    "    safe_name = name.lower().replace(\" \", \"_\").replace(\"(\", \"\").replace(\")\", \"\").replace(\"/\", \"_\")\n",
    "    joblib.dump(obj[\"model\"], f\"model/{safe_name}.pkl\")\n",
    "\n",
    "print(\"Saved models + scaler into ./model/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efe35de",
   "metadata": {},
   "source": [
    "## 12) Placeholder: Observations (Write in Markdown)\n",
    "\n",
    "Create an observation table later for README:\n",
    "\n",
    "| Model | Observation |\n",
    "|------|-------------|\n",
    "| Logistic Regression | ... |\n",
    "| Decision Tree | ... |\n",
    "| KNN | ... |\n",
    "| Naive Bayes | ... |\n",
    "| Random Forest | ... |\n",
    "| XGBoost | ... |\n",
    "\n",
    "Focus on:\n",
    "- Which performed best and why (AUC/MCC/F1)\n",
    "- Which underperformed and why (e.g., NB assumptions)\n",
    "- Impact of scaling on LR/KNN\n",
    "- Any overfitting signs (Tree vs Ensembles)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d541e996",
   "metadata": {},
   "source": [
    "## 13) Notes for Streamlit `app.py` (Planning)\n",
    "\n",
    "Your Streamlit app should:\n",
    "- Allow user to upload **CSV test data** (include target column so you can compute metrics)\n",
    "- Allow selecting a model from dropdown\n",
    "- Load model from `model/*.pkl`\n",
    "- If model requires scaling: load `scaler.pkl` and transform numeric features before predict\n",
    "- Show:\n",
    "  - Metrics (Accuracy, AUC, Precision, Recall, F1, MCC)\n",
    "  - Confusion matrix or classification report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b19d4bc",
   "metadata": {},
   "source": [
    "## 14) Final Submission Checklist (Before you submit)\n",
    "\n",
    "- [ ] Dataset meets **≥ 500 rows** and **≥ 12 features**\n",
    "- [ ] All **6 models** trained on the **same dataset**\n",
    "- [ ] Metrics computed: Accuracy, AUC, Precision, Recall, F1, MCC\n",
    "- [ ] `results_df` comparison table ready to paste into README\n",
    "- [ ] Observation table filled (model → observation)\n",
    "- [ ] Models + scaler saved into `model/`\n",
    "- [ ] `requirements.txt` created and tested\n",
    "- [ ] Streamlit app (`app.py`) runs locally without errors\n",
    "- [ ] Deployed on Streamlit Community Cloud and link works\n",
    "- [ ] Ran on BITS Virtual Lab and captured **one screenshot**\n",
    "- [ ] Final PDF created with required order: GitHub link → Streamlit link → screenshot → README content\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
